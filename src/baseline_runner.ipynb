{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import optuna\n",
    "import torch as pt\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from utils.data_loading import CachedBatches, dataset_loaders\n",
    "from utils.git_and_reproducibility import *\n",
    "from utils.model_operations import *\n",
    "from utils.training import MockTrial, loss_fns, set_seeds\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    # Model/data configs\n",
    "    model_id=\"EleutherAI/pythia-14m\",\n",
    "    forget_set_name=\"python\",\n",
    "    ret_lora_config=dict(\n",
    "        # lora_dropout=0.1,\n",
    "        target_modules=[\"dense_h_to_4h\", \"dense_4h_to_h\", \"query_key_value\", \"dense\"],\n",
    "    ),\n",
    "    # Training constants\n",
    "    unlearn_steps=300,\n",
    "    batch_size=16,\n",
    "    # Relearning params\n",
    "    relearn_steps=100,\n",
    "    relearn_lr=3e-4,\n",
    "    eval_batch_size=16,\n",
    "    relearn_lora_conf=dict(r=1, target_modules=\"all-linear\", lora_dropout=0.1),\n",
    ")\n",
    "\n",
    "pt.set_default_device(\"cuda\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s  %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "# load datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_id)\n",
    "retain_set = dataset_loaders[\"wikitext\"](tokenizer)\n",
    "forget_set = dataset_loaders[config.forget_set_name](tokenizer)\n",
    "retain_batches = CachedBatches(retain_set[\"train\"], config.batch_size)\n",
    "forget_batches = CachedBatches(forget_set[\"train\"], config.batch_size)\n",
    "retain_val_batches = CachedBatches(retain_set[\"validation\"], config.eval_batch_size)\n",
    "forget_val_batches = CachedBatches(forget_set[\"validation\"], config.eval_batch_size)\n",
    "r_eval_batch = next(retain_val_batches.fresh_iterator())\n",
    "f_eval_batch = next(forget_val_batches.fresh_iterator())\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(config.model_id)\n",
    "init_forget = eval_loss(base_model, f_eval_batch)\n",
    "init_retain = eval_loss(base_model, r_eval_batch)\n",
    "logging.info(f\"init forget: {init_forget:6.2f}    init retain: {init_retain:6.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearn_lr = 0.05\n",
    "\n",
    "set_seeds(42)\n",
    "# prepare data iterators\n",
    "forget_iter = forget_batches.fresh_iterator()\n",
    "retain_iter = retain_batches.fresh_iterator()\n",
    "# load model (copy from memory for speed)\n",
    "model = deepcopy(base_model)\n",
    "assert all(param.requires_grad for param in model.parameters())\n",
    "\n",
    "# initialize optimizers\n",
    "# SGD is faster and more predictable than Adam\n",
    "# todo, change to Adam?\n",
    "optimizer = pt.optim.SGD(model.parameters(), lr=unlearn_lr)\n",
    "\n",
    "# %\n",
    "# ! unlearning loop\n",
    "logging.info(\"step      base_f      base_r\")\n",
    "for step in range(1, 1 + config.unlearn_steps):\n",
    "    model.train()\n",
    "    f_input_ids = next(forget_iter)\n",
    "    r_input_ids = next(retain_iter)\n",
    "\n",
    "\n",
    "\n",
    "    # ! retain with helper lora\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    output = model(r_input_ids, output_hidden_states=True)\n",
    "    loss = loss_fns[\"cross_entropy\"](output, r_input_ids)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ! unlearn on the base model\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    output = model(f_input_ids, output_hidden_states=True)\n",
    "    # todo change negative cross entropy to entropy\n",
    "    loss = loss_fns[\"negative_cross_entropy\"](output, f_input_ids)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    # ! eval\n",
    "    if step % 10 == 0:\n",
    "        res = {}\n",
    "        res[\"base_forget\"] = eval_loss(model, f_eval_batch)\n",
    "        res[\"base_retain\"] = eval_loss(model, r_eval_batch)\n",
    "\n",
    "        logging.info(f\"{step:4} \" + \" \".join(f\"{v:11.2f}\" for v in res.values()))\n",
    "\n",
    "        # # prune if base forget loss doesn't improve\n",
    "        # if step >= 30 and res[\"base_forget\"] < init_forget + 0.5:\n",
    "        #     logging.info(\"Forget loss stalled\")\n",
    "        #     raise optuna.TrialPruned()\n",
    "        # # prune if nan\n",
    "        # if any(pt.isnan(v) for v in res.values()):\n",
    "        #     logging.error(\"NaN in eval results\")\n",
    "        #     raise optuna.TrialPruned()\n",
    "        # # prune if retain loss is too high\n",
    "        # if res[\"base_retain\"] > init_retain + 0.1:\n",
    "        #     logging.info(\"Retain performance still broken\")\n",
    "        #     raise optuna.TrialPruned()\n",
    "\n",
    "# %\n",
    "# ! final bigger eval relearning\n",
    "copied_model = deepcopy(model)\n",
    "retain_val_iter = retain_val_batches.fresh_iterator()\n",
    "forget_val_iter = forget_val_batches.fresh_iterator()\n",
    "forget_loss = relearn(copied_model, config, retain_val_iter, forget_val_iter)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
