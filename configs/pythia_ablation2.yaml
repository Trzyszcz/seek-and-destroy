general_config:
  method_name: surgical_tar
  target_modules:
    - dense_h_to_4h
  unlearning_loss_fn: correct_logit_minus_avg
  model_id: EleutherAI/pythia-14m
  # model_id: HuggingFaceTB/SmolLM-135M
  retain_set_name: wikitext
  forget_set_name: python
  unlearn_steps: 600
  batch_size: 16
  n_trials: 200
  # method specific
  use_masking: true
  use_normalization: true
  train_adversary: true
  additional_param_name: null

relearn_config:
  relearn_steps: 300
  relearn_lr: 1.0e-4
  relearn_lora_conf:
    target_modules: all-linear

hyperparams:
  adv_lr: [0.001, 0.005, true]
  adv_decay: [0.5, 1, false]
  retain_momentum_decay: [0, 0.99, false]
  fork_every_n_loops: [6, 42, false]
  retaining_rate: [5.e-4, 5.e-3, true]
  unlearning_rate: [3.e-3, 7.e-2, true]
  additional_param: [0, 0, false]  # on default don't use this
  # todo repE retaining rate

variants:
  surgical_tar: {}

  # ! ablations
  no_r_momentum:
    retain_momentum_decay: [0, 0, false]
  no_adv_decay:
    adv_decay: [1, 1, false]
  neg_entropy:
    unlearning_loss_fn: neg_entropy
  neg_cross_entropy:
    unlearning_loss_fn: neg_cross_entropy
  no_masking:
    use_masking: false
  no_normalization:
    use_normalization: false
    # set manually to keep a similar update scale across variants
    update_scale_factor: 0.15
    # unlearning_rate should actually be similar
  no_adversary:
    train_adversary: false

  # ! optional components
  f_momentum:
    additional_param_name: forget_momentum_decay
    additional_param: [0, 1, false]
  adv_update:
    additional_param_name: adv_update
    additional_param: [0, 1, false]
  clip_at:
    additional_param_name: clip_at
    additional_param: [-10, 10, false]
  discard_growing_weights:
    additional_param_name: discard_growing_weights
    additional_param: [0, 1, false]
  # todo full + repE loss
  
  surgical_tar_lora:
    method_name: surgical_tar_lora
    lora_amount: 1
    lora_rank: 8
    adv_lr: [0.001, 0.05, true]  # LoRA can have a higher learning rate
  tar:
    method_name: tar
    update_scale_factor: 0.15
    adv_decay: [1, 1, false]  # has no effect here, so set it to 1
    unlearning_rate: [1.e-3, 5.e-2, true]
    # todo needs also repE loss
  # todo TAR that can use safeguarding step?
    
  # circuit_breaker:
  #   # note: it's trained without LoRA as was the original circuit breaker paper
  #   forget_momentum_decay: [0, 0, false]
  #   retain_momentum_decay: [0, 0, false]
  #   adv_decay: [1, 1, false]
  #   adv_update: [0, 0, false]
  #   use_masking: false
  #   use_normalization: false
  #   update_scale_factor: 0.1
  #   unlearning_rate: [1.e-3, 5.e-2, true]
  #   train_adversary: false
  #   # todo needs also repE forget loss
  #   # todo needs also repE retain loss
    
  
  # todo tuning unlearning_rate per module? - separate experiment with SmolLM
  # todo only decrease weights
